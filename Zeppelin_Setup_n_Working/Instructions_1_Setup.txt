#Setup Zeppelin with Hadoop and Spark Integrated
------------------------------------------

Refer this link to setup spark on windows
https://github.com/ajaykuma/BigData_HESS/blob/main/Spark_n_Scala/SetupSparkonWindows.txt

for datasets:
https://github.com/ajaykuma/datasets

once done..

download zeppelin from https://zeppelin.apache.org/download.html (look in old releases section)

zeppelin-0.8.2-bin-all.tgz

--unpack the downloaded file
--go into I:\zeppelin-0.8.2-bin-all\conf folder and change the template files 
shiro.ini.template -- shiro.ini
zeppelin-env.cmd.template -- zeppelin-env.cmd ( windows command script file)
zeppelin-env.sh.template -- zeppelin-env.sh ( Shell script)
zeppelin-site.xml.template -- zeppelin-site.xml (xml document)

copy spark jars  into zeppelin spark folder
$ cp %SPARK_HOME%\jars\*.jar  %ZEPPELIN_HOME%\interpreter\spark
delete datanucleus*.jar from %ZEPPELIN_HOME%\interpreter\spark\dep

copy pyspark from existing Spark installation

$ cp %SPARK_HOME%\python\lib\*.zip  %ZEPPELIN_HOME%\interpreter\spark\pyspark

Additional you can copy jars from
%ZEPPELIN_HOME%\interpreter\lib
%ZEPPELIN_HOME%\interpreter\lib\interpreter

into 
%ZEPPELIN_HOME%\interpreter\spark\

--update the following in zeppelin-env.cmd file ( to point to hadoop and spark)
set JAVA_HOME=C:\Java\jdk1.8.0_221
set SPARK_HOME=C:\spark-2.4.3-bin-hadoop2.6
set HADOOP_CONF_DIR=C:\Users\Win10\Desktop\hadoop
set PYTHONPATH=%SPARK_HOME%\python;%SPARK_HOME%\python\lib\py4j-0.10.7-src.zip;%SPARK_HOME%\python\lib\pyspark.zip

--check in shiro.ini and update/set password for admin user

--go into I:\zeppelin-0.8.2-bin-all\bin and run
zeppelin (windows command) [preferably as administrator]
--Look for 
 INFO [2021-02-16 08:17:12,257] ({main} ContextHandler.java[doStart]:855) - Started o.e.j.w.WebAppContext@5ad851c9{zeppelin-web,/,file:///I:/zeppelin-0.8.2-bin-all/webapps/webapp/,AVAILABLE}{I:\zeppelin-0.8.2-bin-all\zeppelin-web-0.8.2.war}
 INFO [2021-02-16 08:17:12,287] ({main} AbstractConnector.java[doStart]:292) - Started ServerConnector@149b0577{HTTP/1.1,[http/1.1]}{127.0.0.1:8080}
 INFO [2021-02-16 08:17:12,288] ({main} Server.java[doStart]:407) - Started @37102ms
 INFO [2021-02-16 08:17:12,289] ({main} ZeppelinServer.java[main]:249) - Done, zeppelin server started


access zeppelin
http://127.0.0.1:8080

#Testing zeppelin

< from right top menu > interpreter
--search for spark
master: local[*]    
#this can later be changed to yarn-cluster to integrate zeppelin with spark & hadoop's yarn
spark.app.name: zeppelin
#this can be customized
zeppelin.pyspark.python:python
#points to python of your machine

--create a notebook

#using spark with scala
#using RDD API
val x = sc.parallelize(1 to 10)
val y = x.map(n => n * 100)
val z = y.filter(n=> n%50 == 0)
z.collect
--------------------
#using spark and python
#using RDD API
%pyspark
x = sc.textFile("I:\\GitContent\\Datasets\\Datasets\\cv000_29416.txt")
x

%pyspark
y = x.map(lambda n : n.upper())
y

%pyspark
z = y.filter(lambda n: n.contains("THE"))

%pyspark
z.collect()

%pyspark
z.first()

--------------------
#using dataframe API
%pyspark
x = spark.read.json("I:\\GitContent\\Datasets\\Datasets\\primer-dataset2.json")
x.show(2)

%pyspark
x.printSchema()

%pyspark
x.select("name","cuisine").show(10)

%pyspark
x.filter($"cuisine" === "Hamburgers").show(5)

#using spark with python
%pyspark
x.filter("cuisine = 'Hamburgers'").show(5)








